{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 441 Final Project - Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (6090, 5)\n",
      "Validation set: (1523, 5)\n",
      "Example:\n",
      " id                                                         49\n",
      "keyword                                                ablaze\n",
      "location                        Est. September 2012 - Bristol\n",
      "text        We always try to bring the heavy. #metal #RT h...\n",
      "target                                                      0\n",
      "Name: 32, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Split the dataset (80% train, 20% validation)\n",
    "train_data, val_data = train_test_split(train_df, test_size = 0.2, random_state = 441)\n",
    "\n",
    "print(\"Training set:\", train_data.shape)\n",
    "print(\"Validation set:\", val_data.shape)\n",
    "\n",
    "print(\"Example:\\n\", train_df.iloc[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: British diver Neil Anthony Fears found dead by the wreck of a steamship - Daily Mail http://t.co/QP3GVvfoFq\n",
      "Processed: british diver neil anthony fear found dead wreck steamship daily mail http\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text, remove part of speech, stopwords, and http links\n",
    "# https://stackoverflow.com/questions/17390326/getting-rid-of-stop-words-and-document-tokenization-using-nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "train_data[\"clean_text\"] = train_data[\"text\"].apply(preprocess_text)\n",
    "val_data[\"clean_text\"] = val_data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(train_data[\"clean_text\"])\n",
    "y_train = train_data[\"target\"]\n",
    "\n",
    "X_val = tfidf_vectorizer.transform(val_data[\"clean_text\"])\n",
    "y_val = val_data[\"target\"]\n",
    "\n",
    "print(\"Original:\", train_data.iloc[1][\"text\"])\n",
    "print(\"Processed:\", train_data.iloc[1][\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3790)\t0.4528132146058555\n",
      "  (0, 4188)\t0.6304602258185331\n",
      "  (0, 1052)\t0.6304602258185331\n",
      "  (1, 2111)\t0.0783485762315847\n",
      "  (1, 2629)\t0.3755054057380762\n",
      "  (1, 1074)\t0.3234361332079501\n",
      "  (1, 4938)\t0.2867904162218649\n",
      "  (1, 1105)\t0.25773202663579503\n",
      "  (1, 1741)\t0.2844201476156351\n",
      "  (1, 1603)\t0.2696431982036788\n",
      "  (1, 170)\t0.4083574590241537\n",
      "  (1, 2921)\t0.4083574590241537\n",
      "  (1, 565)\t0.3353473047758339\n",
      "  (2, 2597)\t0.49900367146439967\n",
      "  (2, 2027)\t0.6185146148096988\n",
      "  (2, 3788)\t0.6069884736400349\n",
      "  (3, 384)\t0.25601239218389893\n",
      "  (3, 1831)\t0.15656023521192458\n",
      "  (3, 2929)\t0.20923834054253226\n",
      "  (3, 4919)\t0.1840471674116045\n",
      "  (3, 4858)\t0.22375717070468867\n",
      "  (3, 2227)\t0.24435971857921673\n",
      "  (3, 2456)\t0.22853864159345988\n",
      "  (3, 1696)\t0.5467947578070591\n",
      "  (3, 3248)\t0.46811688928390544\n",
      "  :\t:\n",
      "  (6086, 781)\t0.7445823875382646\n",
      "  (6086, 2102)\t0.25512528788254585\n",
      "  (6086, 1247)\t0.2903709775038372\n",
      "  (6086, 990)\t0.24308554360957427\n",
      "  (6086, 2873)\t0.2886177297671586\n",
      "  (6086, 2111)\t0.07389615676752616\n",
      "  (6087, 3081)\t0.5020199572795783\n",
      "  (6087, 1227)\t0.4309718159144017\n",
      "  (6087, 4996)\t0.3766733061624557\n",
      "  (6087, 933)\t0.4775789271147468\n",
      "  (6087, 3047)\t0.2819619389379647\n",
      "  (6087, 4786)\t0.32069140254217077\n",
      "  (6087, 2111)\t0.09964604611776971\n",
      "  (6088, 2779)\t0.4196700428656946\n",
      "  (6088, 4316)\t0.4523588593946321\n",
      "  (6088, 100)\t0.44133414290219275\n",
      "  (6088, 4801)\t0.3720111626512034\n",
      "  (6088, 3747)\t0.4275365817119944\n",
      "  (6088, 1831)\t0.3006226511552751\n",
      "  (6088, 2111)\t0.11366124902699687\n",
      "  (6089, 115)\t0.4561850471385065\n",
      "  (6089, 1833)\t0.7272272874636048\n",
      "  (6089, 2000)\t0.3906553294811405\n",
      "  (6089, 4919)\t0.316342587974428\n",
      "  (6089, 2111)\t0.1017421039549944\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
